# -*- coding: utf-8 -*-
"""hw2_student_version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jCKjmQ53GUr_WncVTG79XCqPLD8xZfkw

## Part 0: Libraries and Set Up Jargon (The usual wall of imports)

# CIS 5450 Homework 2: SQL
## Due: Tuesday, February 21 2023, 10pm EST 
### Worth 100 points in total

Welcome to Homework 2! By now, you should be familiar with the world of data science and the Pandas library. This assignment focuses on helping you get to grips with a new tool: SQL.

Through this homework, we will be working with SQL (specifically **pandasql**) by exploring a [Indego](https://www.rideindego.com/) dataset containing bike rides, stations and weather data. We will also conduct some text analysis of reataurant reviews in Philly.

 <!-- We will finish off the homework with some text analysis. -->

We are introducing a lot of new things in this homework, and this is often where students start to get lost. Thus, we **strongly** encourage you to review the slides/material as you work through this assignment. 

**Before you begin:**
- Be sure to click "Copy to Drive" to make sure you're working on your own personal version of the homework
- Check the pinned FAQ post on Ed for updates! If you have been stuck, chances are other students have also faced similar problems.

# <ins> Note</ins>: this has historically been the _**MOST  difficult**_ HW out of all of them, so please watch the SQL recitation and **start early**!
"""

!pip3 install penngrader
!pip install sqlalchemy==1.4.46
!pip install pandasql
!pip install geopy
!pip install -U kaleido

from penngrader.grader import *
import pandas as pd
import datetime as dt
import geopy.distance as gp
import matplotlib.image as mpimg
import plotly.express as px
# import re
import pandasql as ps #SQL on Pandas Dataframe
import nltk
nltk.download('punkt')

from wordcloud import WordCloud

import matplotlib.pyplot as plt 
# from collections import Counter
# import random

# Three datasets we're using
! wget -nc https://storage.googleapis.com/penn-cis5450/indego_trips.csv
! wget -nc https://storage.googleapis.com/penn-cis5450/indego_stations.csv
! wget -nc https://storage.googleapis.com/penn-cis5450/weather_2022_PHL.csv
! wget -nc https://storage.googleapis.com/penn-cis5450/restaurant_reviews.csv

print(pd.__version__ )

"""### PennGrader Setup"""

# ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW 
# TO ASSIGN POINTS TO YOU IN OUR BACKEND

STUDENT_ID = 51747601                                                           # YOUR PENN-ID GOES HERE AS AN INTEGER #


################################################################################
# Check (no need to modify below - we're checking if you changed the ID above)
if STUDENT_ID == 12345678:
   print("Hold up!!! \nHold up!!!!! \nHold up!!!!!!!! \n \nWe can't store your grade if you forgot to change the ID!")

grader = PennGrader(homework_id = 'CIS5450_23Sp_HW2', student_id = STUDENT_ID)

"""# Biking in Philadelphia

<br>
<center><img src = "https://whyy.org/wp-content/uploads/2021/11/AP-indego-bike-share-philly-docking-station-110421-768x512.jpg" width= "500" align ="center"/></center>
<br>

I'm sure in your time in Philadelphia so far you've come across these blue bikes and stations. Indego is the company responsible for this bike sharing ride system, and they make data on bike trips available to the public. This data can not only be useful to get information of how people in Philly use bikes, but it can give information on the most visited places in the city which can be useful for city planners and business owners.


In this homework, we'll be exploring some data about bikes including:

*   Trips: data about bike trips during the first week of October 2022.

*   Stations: data about bike stations, their ID and Name.

*   Weather: data about the weather in Philadelphia during 2022.


We'll be parsing this data into dataframes and relations, and then exploring how to query and assemble the tables into results. We will primarily be using PandaSQL, but for some of the initial questions, we will ask you to perform the same operations in Pandas as well, so as to familiarize you with the differences and similarities of the two.

For the final part of this Homework, we'll use a fourth database with restaurants and reviews, and perform some text analysis.

## Part 1: Load & Process our Datasets [10 points total]

Before we get into the data, we first need to load and clean our datasets. 

# Metadata
You'll be working with four CSV files:
- `indego_trips.csv`: contains data about each trip, like the origin station, destination station and duration. 
- `indego_stations.csv`: includes information about stations and their status in January 2023.
- `weather_2022_PHL.csv`: has one row per day during 2022 and shows weather information.
- `restaurant_reviews.csv`: has several reviews about three restaurants.

**TO-DO**:
* Load `indego_trips.csv` and save the data to a dataframe called `trips_df`.
* Load `indego_stations.csv` and save the data to a dataframe called `stations_df`.
* Load `weather_2022_PHL.csv` and save the data to a dataframe called `weather_df`.
* Load `restaurant_reviews.csv` and save the data to a dataframe called `reviews_df`.
"""

# TODO: Import the 4 datasets to pandas dataframes -- make sure the dataframes are named correctly! 
trips_df = pd.read_csv("indego_trips.csv")
stations_df = pd.read_csv("indego_stations.csv")
weather_df = pd.read_csv("weather_2022_PHL.csv")
reviews_df = pd.read_csv("restaurant_reviews.csv")

# TODO: view `trips_df` (just the first 5 rows with .head() is fine) to make sure the import was successful
trips_df.head(5)

# TODO: view `stations_df` (just the first 5 rows with .head() is fine)to make sure the import was successful
stations_df.head(5)

# TODO: view `weather_df` (just the first 5 rows with .head() is fine)to make sure the import was successful
weather_df.head(5)

# TODO: view `reviews_df` (just the first 5 rows with .head() is fine)to make sure the import was successful
reviews_df.head(5)

"""### 1.1 Data Preprocessing

Next, we are going to want to clean up our dataframes, namely `trips_df` and `stations_df`, by 

1.   Fixing columns
2.   Changing datatypes
3.   Handling nulls


First, let us view the first few rows of `trips_df`. You may also call `.info()` and additionally check the cardinality of each column to view the specifics of the dataframe. This is a good first step to take for Exploratory Data Analysis (EDA).
"""

# view the .info() information for trips_df
trips_df.info()

stations_df.info()

"""#### 1.1.1 Cleaning `trips_df` (4 points)

`.info()` gives us meaningful information regarding columns, their types, and the amount of nulls, based on which we can now clean our dataframe. 

Perform these steps and save results on a new dataframe: `trips_cleaned_df` 

**TODO**:
* Drop the column `plan_duration`. We already have that information in the column `passholder_type`, which is more understandable.
* Drop the rows where `end_station` is 3000. This is a virtual station used for maintainance, and doesn't represent a real trip.
* Drop all rows with null values.
* Cast the columns:
  -  `start_time`, `end_time`, `trip_route_category`, `passholder_type`, `bike_type` as **string**. (Cast to 'string' and not 'str') 
  - `bike_id` as **int**.
* Save results as `trips_cleaned_df` and sort it by `trip_id` ascending

After performing these steps, `trips_cleaned_df` should have the following schema:

**Final Schema**:
>trip_id | duration | start_time | end_time | start_station | start_lat | start_lon | end_station |  end_lat | end_lon |  bike_id | trip_route_category | passholder_type | bike_type
>--- | --- | --- |--- | --- | --- | --- |--- | --- | --- |--- |--- | --- | --- |
"""

# TO-DO: drop the plan_duration column
trips_cleaned_df = trips_df.drop(["plan_duration"], axis=1)

# TO-DO: drop rows with end_station = 3000
trips_cleaned_df = trips_cleaned_df[trips_cleaned_df.end_station != 3000]

# TO-DO: drop nulls
trips_cleaned_df = trips_cleaned_df.dropna()

# TO-DO: cast columns as indicated types
trips_cleaned_df["start_time"] = trips_cleaned_df["start_time"].astype("string")
trips_cleaned_df['end_time'] = trips_cleaned_df['end_time'].astype("string")
trips_cleaned_df['trip_route_category'] = trips_cleaned_df['trip_route_category'].astype("string")
trips_cleaned_df['passholder_type'] = trips_cleaned_df['passholder_type'].astype("string")
trips_cleaned_df['bike_type'] = trips_cleaned_df['bike_type'].astype("string")
trips_cleaned_df['bike_id'] = trips_cleaned_df['bike_id'].astype(int)

#TO-DO: save changes to trips_cleaned_df that is sorted by 'trip_id' 
trips_cleaned_df = trips_cleaned_df.sort_values(["trip_id"])

# 4 points
grader.grade(test_case_id = 'test_cleaning_trips', answer = [len(trips_cleaned_df), trips_cleaned_df.head()])

"""#### 1.1.2 Processing Stations (3 points)

`stations_df` contains information on Indego stations across the city. We will clean this df by removing Inactive stations and stations created after October 2022.

Perform these steps and assign the cleaned dataframe to `stations_cleaned_df`.

**TODO**:
* **Cast** column `day_of_go_live_date` as datetime64[ns].
* **Drop** the stations that were created **after 10/7/2022** (i.e. we want to keep the ones on or before 10/7/2022)
* **Drop** the stations that have an **Inactive** status.
* **Drop** the columns `day_of_go_live_date` and `status`
* **Create** a new column called `is_west_philly` that is True if zone is 2 or 3 and False otherwise.
* **Save** the resulting dataframe as `stations_cleaned_df`, and sort it by `station_id` ascending


After performing these steps, `stations_cleaned_df` should have the following schema:

**Final Schema**:
>station_id | station_name | zone | is_west_philly
>--- | --- | --- |--- |
"""

from numpy import datetime64
# TO-DO: Your code goes below. We recommend creating code comments for each of the bullets above (like we did for you in 1.1.1)
stations_cleaned_df = stations_df.copy()
stations_cleaned_df["day_of_go_live_date"] = stations_cleaned_df["day_of_go_live_date"].astype(datetime64)
stations_cleaned_df = stations_cleaned_df[stations_cleaned_df["day_of_go_live_date"] <= "10/7/2022"]
stations_cleaned_df = stations_cleaned_df[stations_cleaned_df.status != "Inactive"]
stations_cleaned_df = stations_cleaned_df.drop(["day_of_go_live_date", "status"], axis=1)
stations_cleaned_df["is_west_philly"] = stations_cleaned_df.apply(lambda x: True if x.zone == 2 else False, axis=1)
stations_cleaned_df = stations_cleaned_df.sort_values(["station_id"])

# 3 points
grader.grade(test_case_id = 'test_stations_processing', answer = (len(stations_cleaned_df), stations_cleaned_df.head()))

"""#### 1.1.3 Cleaning the weather (3 points)
Then, let's clean `weather_df` and make it usable.
"""

# view .info() of weather_df to sense check it
weather_df.info()

"""
**TO-DO**:
* Create `weather_cleaned_df` and only keep the following 5 columns:
     * `date`, `actual_mean_temp`, `actual_min_temp`, `actual_max_temp`, `actual_precipitation`
* Convert column `date` into type `datetime64[ns]`.
* Keep only the rows from 9/1/2022 to 10/31/2022, inclusive.
* Sort by column `date` descending.

After performing these steps, `weather_cleaned_df` should have the following schema:

**Final Schema**:
>date | actual_mean_temp | actual_min_temp | actual_max_temp | actual_precipitation
>--- | --- | --- |--- |--- |"""

#TO-DO: clean up the weather_df 

weather_cleaned_df = weather_df[["date", "actual_mean_temp","actual_min_temp","actual_max_temp","actual_precipitation"]]
weather_cleaned_df["date"] = weather_cleaned_df["date"].astype(datetime64)
weather_cleaned_df = weather_cleaned_df["9/1/2022" <= weather_cleaned_df["date"]]
weather_cleaned_df = weather_cleaned_df[weather_cleaned_df["date"] <= "10/31/2022"]
weather_cleaned_df = weather_cleaned_df.sort_values(["date"], ascending=False)

# 3 points
grader.grade(test_case_id = 'test_cleaning_weather', answer = [len(weather_cleaned_df),weather_cleaned_df.head()])

"""### 1.2 Your Sandbox 

`.info()` is just one of many basic tools that you can use for Exploratory Data Analysis (EDA). Instead of throwing you straight into the deep end, we wanted to give you a chance to take some time and explore the data on your own. **This section is not graded**, so for the speedrunners out there feel free to just jump in, but we wanted to at least give you a small space to utilize your EDA toolkit to familiarize yourself with all the data you just downloaded.

Some suggestions to get you started:
- `df.head()`
- `df.describe()`
- `Series.unique()`
"""

# Your EDA here! Feel free to add more cells
trips_cleaned_df.head()

"""## Part 2: Exploring the Data with PandasSQL (and Pandas) [73 points total]

Now that you are familiar (or still unfamiliar) with the dataset, we will now introduce you to SQL, or more specifically **pandasql**: a package created to allow users to query pandas DataFrames with SQL statements.

## 👇👇👇 IMPORTANT: Pay VERY CLOSE attention to this style guide! 👇👇👇

The typical flow to use pandasql (aliased as `ps`) is as follows:
1. Write a SQL query in the form of a string
    - **String Syntax:** use triple quotes `'''<your query>'''` to write multi-line strings in Python
    - **Aliases are your friend:** if there are very long table names or you find yourself needed to declare the source (common during join tasks), it's almost always optimal to alias your tables with short INTUITIVE alias names
    - **New Clauses New Line:** each of the main SQL clauses (`SELECT`, `FROM`, `WHERE`, etc.) should begin on a new line
    - **Use Indentation:** if there are many components for a single clause, separate them out with new <ins>indented</ins> lines.

    Example below:
    ```SQL
    '''
    SELECT ltn.some_id, SUM(stn.some_value) AS total
    FROM long_table_name AS ltn
         INNER JOIN short_table_name AS stn 
            ON ltn.common_key = stn.common_key
         INNER JOIN med_table_name AS mtn
            ON ltn.other_key = mtn.other_key
    WHERE ltn.col1 > value
         AND stn.col2 <= another_value
         AND mtn.col3 != something_else
    GROUP BY ltn.some_id
    ORDER BY total DESC
    '''
    ```
2. Run the query using `ps.sqldf(your_query, locals())`

Pandasql is convenient in that it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the dataframes `trips_cleaned_df`, `stations_cleaned_df` and `weather__cleaned_df` that you have created above!

Given that it is a brand new language, we wanted to give you a chance to directly compare the similarities/differences of the pandas that you already know and the SQL that you are about to learn. Thus, for each of the simpler queries, we may ask that you **look into the question twice: once with pandas and once with pandasql**.

### 2.1 One Way vs Round Trip [19 points]

#### 2.1.1 How many of the rides taken were Round Trip journeys? (4 points)

The dataframe `trips_cleaned_df` contains information for each ride. We want to know which of these rides were Round Trip journeys, and how many such rides were taken.

**TO-DO:**

*    Using what you learned in EDA in 1.2, use `pandas` to filter on the appropriate column to obtain only round trips, and save it to a DataFrame called `round_df`, which is sorted by `trip_id` in ascending order.
*   Save the number of rides in the integer variable `number_of_rounds`.
"""

# TO-DO: Use pandas to obtain rides that were round trip
round_df = trips_cleaned_df[trips_cleaned_df["trip_route_category"] == "Round Trip"]
round_df = round_df.sort_values(["trip_id"])

# TO-DO : Save the number of round trip rides obtained from round_df to number_of_rounds
number_of_rounds = len(round_df)
number_of_rounds

# 2 points
grader.grade(test_case_id = 'test_round', answer = (round_df, number_of_rounds))

"""**TO-DO:** Now using **pandasql**, calculate the number of round trips in trips_cleaned_df and save it to the dataframe `round_df_sql`, with the column named `num`. This will be a rather unconventional 1x1 dataframe, but that's OK.

**Final Schema:**

> num|
> --- |
"""

# TODO: Use pandasql to calculate the number of round trips
round_query = """
              SELECT COUNT(*) AS num
              FROM trips_cleaned_df AS trips
              WHERE trips.trip_route_category == "Round Trip"
              """
round_df_sql = ps.sqldf(round_query, locals())
round_df_sql

# 2 points
grader.grade(test_case_id = 'test_round_sql', answer = round_df_sql)

"""#### 2.1.2 What was the duration of these round trips? [4 points]

The dataframe `round_df` contains information for each ride that was a round trip. We want to know the average duration of the trip for all these rides. 

**TO-DO:** Using **pandas**, calculate the average duration of the rides in `round_df` and save it to the variable `avg_round_pd`. Please make sure you do NOT cast it to any type explicitly nor should you round it. 




"""

# TODO: Use pandas to calculate the average duration of round trips
avg_round_pd = round_df.duration.mean()

# 2 points
grader.grade(test_case_id = 'test_round_dur_pd', answer = avg_round_pd)

"""**TO-DO:** Using **pandasql**, calculate the average duration of the rides in round_df and save it to the dataframe `avg_round_df`, with the column named `avg_dur`. Please make sure you do NOT cast it to any type explicitly nor should you round it.

Again, this will be a 1x1 dataframe but that's OK.

**Final Schema:**


> avg_dur |
> --- |
"""

# TODO: Use pandasql to calculate the average duration of round trips
avg_dur_query = """
                SELECT AVG(duration) AS avg_dur
                FROM round_df
                """

avg_round_df = ps.sqldf(avg_dur_query, locals())
avg_round_df

# 2 points
grader.grade(test_case_id = 'test_round_dur', answer = avg_round_df)

"""#### 2.1.3 How long do you take? (5 points)

Some of the rides listed in the `trips_cleaned_df` are marked as Day Pass. We'd like to know how long riders take their bikes out when they get a Day Pass. 

**TO-DO:** Using **pandas**, and what you learned in EDA in 1.2, use a filter on the appropriate column to obtain only “Day Pass” to calculate the average duration of these trips from `trips_cleaned_df` and save it to the variable `day_df_pd`. Please make sure you do not cast it to any type explicitly, or round it.  

**Final Schema:**

> avg_dur |
> --- |

Please use only `trips_cleaned_df` for both `pandas` and `pandasql`.
"""

# TODO: Use pandas to calculate the average duration of trips with a day pass 
day_df_pd = trips_cleaned_df[trips_cleaned_df.passholder_type == "Day Pass"].duration.mean()

"""**TO-DO:** Repeat the same using **pandasql** and save it as `day_df_sql` where the average duration column should be named `avg_dur`."""

# TODO: Use pandasql to calculate the average duration of trips with a day pass 
day_query = """
            SELECT AVG(duration) AS avg_dur
            FROM trips_cleaned_df
            WHERE passholder_type == "Day Pass"
            """

day_df_sql = ps.sqldf(day_query, locals())
day_df_sql

# 4 points
grader.grade(test_case_id = 'test_day_dur', answer = (day_df_sql, day_df_pd, day_query))

"""##### **Reflect:**  What do you think? (1 point)

Is there a difference between the average duration for round trips (2.1.2) and Day Pass journeys (2.1.3)? Please state your answer as Yes/No with a reason for the possible similarity/difference? [1 pt]


"""

your_ans = """Yes, the former one is in category: round trip, the latter is in pass holder type: Day Pass, their calculations are based on different statistics"""

# Grader Cell for Text Response (do not modify) [1 point]
grader.grade(test_case_id = 'test_q', answer = your_ans)

"""#### 2.1.4 Ratio of Rides (6 points)

Some of the rides listed in the `trips_cleaned_df` are marked as One Way while some are marked as Round Trip. We'd like to know the ratio between the number of one - way trips to the number of round-trip journeys. 

**TO-DO:** Using **pandas**, calculate the ratio of the number of one-way rides to the number of round-trip rides from `trips_cleaned_df` and save it to the variable `ratio_1_round` 

**HINT:** Let the answer remain a float value when using pandas. 
"""

# TO-DO: Use pandas to obtain the ratio of average number of one-way trips to average number of round-trip journeys
ratio_1_round = len(trips_cleaned_df[trips_cleaned_df.trip_route_category == "One Way"]) / len(trips_cleaned_df[trips_cleaned_df.trip_route_category == "Round Trip"])
print(len(trips_cleaned_df[trips_cleaned_df.trip_route_category == "One Way"]), len(trips_cleaned_df[trips_cleaned_df.trip_route_category == "Round Trip"]), 14325/930)

# 2 points
grader.grade(test_case_id = 'test_ratio_pd', answer = ratio_1_round)

"""Now, let's try doing the same thing using pandasql, and see if the results match. 

**TODO:** Using **pandasql**, calculate the ratio of the number of one-way rides to the number of round-trip rides from `trips_cleaned_df` and save it to the dataframe `ratio_df`, naming the column `'ratio'`. 

Try to make use of subqueries here! Note, your results should be equivalent (to each other, and the answer, of course) if you have done both parts correctly.

**HINT:** Cast your ratio to `float` type.  

**Final Schema:**

> ratio |
> --- |
"""

# TODO: Use pandasql to obtain the ratio of number of one-way trips to  number of round trips 

ratio_query = """  
              SELECT (CAST(COUNT(*) AS FLOAT) / (round_num + 0.0)) AS ratio
              FROM trips_cleaned_df, (
                SELECT COUNT(*) AS round_num
                FROM trips_cleaned_df
                WHERE trip_route_category == "Round Trip"
              )
              WHERE trip_route_category == "One Way"
              """

ratio_df = ps.sqldf(ratio_query, locals())
ratio_df

# 4 points
grader.grade(test_case_id = 'test_ratio_sql', answer = (ratio_df, ratio_query))

"""###2.2 Station Involvement [16 points]

#### 2.2.1 Names not numbers [5 points]
We will now use `stations_cleaned_df` to modify our `trips_final_df`. If you look at the rides data, you will notice that both start and end station columns have numbers corresponding to the station ID listed. Using the station ID listed in stations_df, we want to include station names instead of station IDs in our `trips_final_df`.

**TO-DO** (`pandas`): 
* Drop records with missing values and update `trips_cleaned_df`
* Create a new dataframe called `trips_final_df` which you'll use after this question. Begin by assigning a **copy** of `trips_cleaned_df` to it.
  * **Hint:** Don't just set them equal right away! Refer to the ```.copy()``` function!
. 
* Replace the IDs in the `'start_station'` column of `trips_final_df` with the corresponding station name. 
* Replace the IDs in the `'end_station'` column of `trips_final_df` with the corresponding station name. 
* If the any station ID cannot be mapped to a name, the name should be set to 'Unknown'
* Sort `trips_final_df` by the trip_id (in ascending order).
"""

# TO-DO : Drop missing values
trips_cleaned_df = trips_cleaned_df.dropna()

# TO-DO : Create a copy named trips_final_df using .copy()
trips_final_df = trips_cleaned_df.copy()

from numpy.lib.twodim_base import tri
# TO-DO: Replace station IDs with station names in trips_final_df. Read ALL the bullets above carefully!
merged_table = stations_cleaned_df[["station_id","station_name"]].merge(trips_final_df, how="right", left_on="station_id", right_on="start_station").drop(["start_station", "station_id"], axis=1)
merged_table = merged_table.rename(columns={"station_name": "start_station"})
merged_table = merged_table.merge(stations_cleaned_df[["station_id","station_name"]], how="left", left_on="end_station", right_on="station_id").drop(["end_station", "station_id"], axis=1)
merged_table = merged_table.rename(columns={"station_name": "end_station"})
merged_table.fillna('Unknown', inplace=True)

# TO-DO: Sort trips_final_df by the trip_id
trips_final_df = merged_table.sort_values("trip_id")
trips_final_df[trips_final_df["start_station"] == "Unknown"]
trips_final_df.iloc[80, :]

# 2 points
grader.grade(test_case_id = 'test_stations', answer = trips_final_df)

"""**TO-DO** : Repeat the same using `trips_cleaned_df` (only for this sql query) and `pandasql`, saving it as `trips_stations_sql`.

<ins>Remember:</ins> If a station ID cannot be mapped to a name, the name should be **NULL**
"""

#TODO : Use pandasql to replace station IDs with station names in trips_cleaned_df and save it as trips_stations_sql
# """
# SELECT ltn.some_id, SUM(stn.some_value) AS total
# FROM long_table_name AS ltn
#    INNER JOIN short_table_name AS stn 
#       ON ltn.common_key = stn.common_key
#    INNER JOIN med_table_name AS mtn
#       ON ltn.other_key = mtn.other_key
# WHERE ltn.col1 > value
#    AND stn.col2 <= another_value
#    AND mtn.col3 != something_else
# GROUP BY ltn.some_id
# ORDER BY total DESC
# """
replace_query = """
                SELECT trip_id, duration, start_time, end_time, start_lat, start_lon, end_lat, end_lon, bike_id, trip_route_category, passholder_type,	bike_type, s1.station_name AS start_station, s2.station_name AS end_station
                FROM trips_cleaned_df t
                  LEFT JOIN stations_cleaned_df s1
                    ON t.start_station = s1.station_id
                  LEFT JOIN stations_cleaned_df s2
                    ON t.end_station = s2.station_id
                """

trips_stations_sql = ps.sqldf(replace_query, locals())#ps.sqldf(replace_query, locals())
trips_stations_sql

#3 points
grader.grade(test_case_id = 'test_stations_sql', answer = (trips_stations_sql, replace_query))

"""#### 2.2.2 Morning Class? (5 points)
We are all pretty familiar with University City, most of you probably live in or commute to this area every day. We want to count the number of people who start out at the University City Station before noon. 

**TO-DO**: 
* The first step involved is creating a new column in `trips_final_df` called `hour` where you will extract the hour of the `start_time` column. You can do this using pandas, and an apply function. 
* The next step is to use `pandasql` to count the number of rides that start at the 'University City Station' before noon (do not include trips with hour 12) and store the result in a dataframe called `ucity_df`, with the column name as `ride_count`.

**Hint**: you may find it helpful to look up the `.Timestamp()` function to understand how to extract the hour of a particular date. 
"""

#TODO : Create a new column called hour
trips_final_df["hour"] = trips_final_df.start_time.apply(lambda x: x.split()[1].split(":")[0]).astype(int)

# Count the number of rides before noon that started at `University City Station`
ucity_query = """ 
              SELECT COUNT(*) AS ride_count
              FROM trips_final_df
              WHERE hour < 12 and start_station = "University City Station"
              """

ucity_df = ps.sqldf(ucity_query, locals())
ucity_df

# 7 points
grader.grade(test_case_id = 'test_ucity', answer = (trips_final_df, ucity_df, ucity_query))

"""#### 2.2.3 Most Popular Stations (6 points)
We now want to see which stations from `trips_final_df` are used most frequently. 

**TODO**: 
* Find the total number of rides for each `start_station`
* Return the top 3 most frequently used stations in a dataframe named `top3_df` where the schema looks like this:

> Station | Count_rides
> --- | ---

**Hint**: Use `pandasql` to do this in ONE query. 
"""

#TODO: Return the top three used stations by number of rides
top3_query = """ 
             SELECT start_station AS Station, COUNT(*) AS Count_rides
             FROM trips_final_df
             GROUP BY start_station
             ORDER BY Count_rides DESC
             LIMIT 3
             """
top3_df = ps.sqldf(top3_query, locals())
top3_df

# 6 points
grader.grade(test_case_id = 'test_freq_stat', answer = (top3_query,top3_df))

"""### 2.3 Determining the Distance [8 points]

We are going to be using a new package called geopy to calculate the distance between the start and end stations for a particular ride. 

`geopy` has been imported as `gp` above, and the `.geodesic()` gives you the distance between two sets of coordinates. We have the coordinates for the start_station and end_station for each ride. 

**TO-DO**: 
* For the trip with `'trip_id': 540422637`, find the latitude and longitudes of both the start and end stations.
* Save the latitude and longitude as **tuples** into variables `coords_1` and `coords_2` for the `'start_station'` and `'end_station'` respectively.
  - Do **NOT** hard code the values: you will get a **ZERO** for the question when we manually review for this!
* Use geopy to calculate the distance **in miles** between the two stations and save it to the variable final_dist. 
"""

# TO-DO: Use pandasql to obtain the latitude and longitude of the start and end stations for ride ID 540422637. 
latlng_query = """  
               SELECT start_lat, start_lon, end_lat, end_lon
               FROM trips_final_df
               WHERE trip_id = 540422637
               """

latlng_df = ps.sqldf(latlng_query, locals())
latlng_df

# TO-DO : Fill in the coordinates using the dataframe that has the resultant coordinates
coords_1 = (latlng_df.loc[0, "start_lat"], latlng_df.loc[0, "start_lon"])
coords_2 = (latlng_df.loc[0, "end_lat"], latlng_df.loc[0, "end_lon"])
print(coords_1, coords_2)

# TO-DO: Use geopy and its appropriate functions to calculate the distance between the two stations 
final_dist = gp.geodesic(coords_1, coords_2).miles
final_dist

# 8 points
grader.grade(test_case_id = 'test_geodist', answer = (coords_1, coords_2, final_dist, latlng_query))

"""### 2.4 Destinations from Amtrak Station [7 points]
As you might imagine, Amtrak Station is a very important location for Indego, as there is a lot of people that either want to get there for a train ride or are exiting from there to visit Philadelphia. Thus, we want to learn about where people tend to go once they leave Amtrak Station.

You will first create this dataset using Pandas, and then SQL (pandasql)

**TO-DO:**
- Using `trips_final_df`, find the top 5 most common end stations where Amtrak Station was the start_station where the duration is **at least 15 minutes**
  - Do **NOT** include Amtrak as an end station! 
  - Note that duration is already measured in minutes, and that Amtrak's station_name is _"Amtrak 30th Street Station"_
- For this Pandas section, name the final dataframe `end_station_df`.
- Ensure that `end_station_df` is first sorted by `Count` in descending order, and then sorted by `end_station` in ascending order.

**Final Schema (make sure you have the same column names and order!)**:
> end_station | Count
> --- | ---
"""

# TODO: Get end_station_df using Pandas
end_station_df = trips_final_df[trips_final_df["start_station"] == "Amtrak 30th Street Station"]
end_station_df = end_station_df[end_station_df["end_station"] != "Amtrak 30th Street Station"]
end_station_df = end_station_df[end_station_df["duration"] >= 15][["trip_id", "end_station"]].groupby("end_station").count().rename(columns={"trip_id": "Count"})
end_station_df.reset_index(inplace=True)
end_station_df = end_station_df.sort_values(["Count", "end_station"], ascending=[False, True])[:5]
end_station_df

"""**TO-DO:**
- Repeat the process but now using pandasql. Please make sure to use `end_station_query` to save your SQL code as this is what will be graded.
- Name the final dataframe `sql_end_station_df`
- Double-check that this looks the exact same as `end_station_df` from the Pandas section above.

**Final Schema:**
> end_station | Count
> --- | ---
"""

# TODO: Get the same exact query, but only using PandaSQL

end_station_query = """
                    SELECT end_station, COUNT(*) AS Count
                    FROM trips_final_df
                    WHERE start_station == "Amtrak 30th Street Station" and end_station != "Amtrak 30th Street Station" and duration >= 15
                    GROUP BY end_station
                    ORDER BY Count DESC, end_station ASC
                    LIMIT 5
                    """

sql_end_station_df = ps.sqldf(end_station_query, locals())
sql_end_station_df

# 7 points
grader.grade(test_case_id = 'test_amtrak', answer = (end_station_df, sql_end_station_df, end_station_query))

"""### 2.5 Weather and Trip Data [15 points]

#### 2.5.1 Trips and Weather Tables [2 points]

We will begin by slightly modifying `trips_final_df` and `weather_cleaned_df`. We eventually want to merge using the `date` column from `weather_cleaned_df` and the `start_time` column from `trips_final_df`. You will note that these two columns do not contain the same information, since `date` does not contain the time.

**We are not concerned about the time, it is up to you to find the best way to alter the `start_time` column to keep just the date and thus make merging possible**.

We strongly recommend creating a new `date` column in `trips_final_df` containing only the day information, as it will make merging the dataframes much easier.

**TO-DO:** \\
- `weather_merge_df`, containing columns `date`, `actual_max_temp` and `actual_precipitation`
- `trips_merge_df`, containing columns `date`,  `bike_type`, `start_station` and `trip_id`
"""

# TODO: Create weather_merge_df and trips_merge_df
trips_final_df["date"] = trips_final_df["start_time"].apply(lambda x: x.split()[0]).astype(datetime64)
weather_merge_df = weather_cleaned_df[["date", "actual_max_temp", "actual_precipitation"]]
trips_merge_df = trips_final_df[["date", "bike_type", "start_station", "trip_id"]]

# 2 points
grader.grade(test_case_id = 'test_merge_df', answer = (weather_merge_df, trips_merge_df))

"""#### 2.5.2 Standard Riding on Spruce (SQL) [3 points]

**TO-DO:**
- Using `trips_merge_df` and `weather_merge_df`, *for each actual max temperature*, find the percentage of rides that were of **standard** bike type on trips where the start_station included **Spruce** in it
  - e.g. "13th & Spruce" OR "15th & Spruce" are 2 potential start_stations that would be included
- Filter so that we only have entries where the **average actual_precipitation** is **less than** 0.3

*   Cast your percentages to integers and name the column **Standard_Percentage**

**Final Schema (make sure you have the same column names and order!)**:
>Temperature | Standard_Percentage |
>--- | --- |
"""

# TO-DO: Create percentage_df (use PandaSQL)
percentage_query = '''
                   SELECT actual_max_temp AS Temperature, SUM(bike_type == "standard") * 100 / COUNT(*) AS Standard_Percentage
                   FROM weather_merge_df AS w
                     INNER JOIN trips_merge_df AS t
                       ON w.date = t.date
                   WHERE start_station like "%Spruce%"
                   GROUP BY actual_max_temp
                   HAVING AVG(actual_precipitation) < 0.3
                   '''

percentage_df = ps.sqldf(percentage_query, locals())
percentage_df

# 3 points
grader.grade(test_case_id = 'test_percentage', answer = (percentage_query, percentage_df))

"""#### 2.5.3 Bike Types by Weather (SQL) [3 points]

Using the dataframes from part 2.5.1, we want to find out which bike type is the most popular during good weather conditions.

**TO-DO:**
- Good weather conditions are defined to be when the temperature (`actual_max_temp`) is **AT OR ABOVE 70 degrees Farenheit**, and the `'actual_precipitation'` is **AT OR BELOW 0.3**.
- Create a dataframe that has columns `Bike Type` for each of the various kinds of bikes that Indego offers, and `Number of Rides` for the number of rides taken by each kind of bike.
- For the final dataframe, there should only be ONE entry, consisting of the bike type with the most rides. Name this dataframe `sql_bike_type_df`.

**NOTE:** When renaming one of the aggregated columns as `Number of Rides`, you can select either `actual_max_temp` or `actual_precipitation`, just be consistent!

**Final Schema (make sure you have the same column names and order!)**:
>Bike Type | Number of Rides |
>--- | --- |
"""

# TODO: Create sql_bike_type_df using PandasSQL

bike_query = """
             SELECT bike_type AS "Bike Type", COUNT(*) AS "Number of Rides"
             FROM weather_merge_df AS w
               INNER JOIN trips_merge_df AS t
                 ON w.date = t.date
             WHERE actual_max_temp >= 70 and actual_precipitation <= 0.3
             GROUP BY bike_type
             ORDER BY "Number of Rides" DESC
             LIMIT 1
             """

sql_bike_type_df = ps.sqldf(bike_query, locals())
sql_bike_type_df

# 3 points
grader.grade(test_case_id = 'test_bikes', answer = (sql_bike_type_df, bike_query))

"""#### 2.5.4 Rainy Days vs Non-Rainy Days [7 points]

Just to get a little more practice with SQL...

**TO-DO:**
- Make a dataframe called `sql_rain_df` using query `rain_query` that finds the total number of trips on Rainy Days vs. Sunny Days, along with the number of days for each day type (rainy, non-rainy)
  - A rainy day is defined as one where the precipitation was **greater than 0**.
  - You will find `WITH (...) AS` and `UNION` commands useful here.

<ins>Sense-Check</ins>: The `'Number of Days'` in your output will necessarily need to sum up to **7** since we only have 7 different days in our bikes dataset.

<ins>Warning</ins>: Please note that you do NOT need to do this with Pandas, **ONLY PANDASQL**! <br> If you do this in Pandas, we will reduce the score to **ZERO** during manual review.


**Final Schema** _(make sure you have the same column names and order!)_:
>Day Type | Number of Rides | Number of Days
>--- | --- | --- |
> Rainy Day | ? | ?
> Sunny Day | ? | ?
"""

# TODO: create sql_rain_df as described above. =

rain_query = """
             WITH merge AS (
               SELECT *
               FROM weather_merge_df w
                 INNER JOIN trips_merge_df t
                   ON w.date = t.date
             ),
             rainy AS (
               SELECT "Rainy Day" AS "Day Type", SUM(c) AS "Number of Rides", COUNT(c) AS "Number of Days"
               FROM (
                 SELECT COUNT(*) AS c
                 FROM merge
                 WHERE actual_precipitation > 0
                 GROUP BY date
               )
             ),
             sunny AS (
               SELECT "Sunny Day" AS "Day Type", SUM(c) AS "Number of Rides", COUNT(c) AS "Number of Days"
               FROM (
                 SELECT COUNT(*) AS c
                 FROM merge
                 WHERE actual_precipitation = 0
                 GROUP BY date
               )
             )
             SELECT *
             FROM rainy 
             UNION
             SELECT *
             FROM sunny 
             """

sql_rain_df = ps.sqldf(rain_query, locals())
sql_rain_df

# 7 points
grader.grade(test_case_id = 'test_rainy', answer = (sql_rain_df,rain_query))

"""### 2.6 Weekend Rush [8 points]

We want to determine the percentage change in the number of rides from the weekend to the weekdays in the first 7 days of October using `pandasql`.



**TODO**: Use `pandasql` to :
* Count the number of rides that take place on the weekend (Saturday, Sunday)
* Count the number of rides that take place M-F.
* Calculate the percentage change for `trips_final_df` and save it in a new dataframe called `perc_df` with the column named `perc_change`. Please cast this column to `float` type You can use the following formula:

$$\frac{ number\_of\_trips\_on\_weekday - number\_of\_trips\_on\_weekend}{number\_of\_trips\_on\_weekend} \times 100$$
"""

# TODO: Use pandasql to get the percentage change between trips on the weekend and trips on the weekday
perc_query = """
             SELECT (weekday - weekend) * 100.0 / weekend AS perc_change
             FROM (
               SELECT COUNT(*) AS weekend
               FROM trips_final_df t
               WHERE strftime('%w', t.date) = '0'
                 OR strftime('%w', t.date) = '6'
             ),
             (
               SELECT COUNT(*) AS weekday
               FROM trips_final_df t
               WHERE strftime('%w', t.date) != '0'
                 AND strftime('%w', t.date) != '6'
             ) 
             """

perc_df = ps.sqldf(perc_query, locals())
perc_df

# 8 points
grader.grade(test_case_id = 'test_perc', answer = (perc_df, perc_query))

"""## Part 3: Data Visualization [8 points total]

You've done great work so far! Now let's create a couple visualizations to illustrate data we might be interested in. 


This section will be **manually graded**.

Begin by following the directions below to prepare the dataset for plotting
"""

from datetime import datetime
import seaborn as sns
import numpy as np

"""For this part we will be using `trips_final_df`"""

# Step 1a - using datetime functions, create a column called Time that has the time 
# (Hour:Minute) seen in the start_time column

# HINT: The date should be of type %m/%d/%Y %H:%M, and once you've converted to datetime
# you can use the .time() function to get the time of day

# datetime.strptime(date_string, format)
trips_final_df['Time'] = trips_final_df['start_time'].apply(lambda x: x.split()[1])
# trips_final_df

# Step 1b - to make things simple for our graph, "round" the time down to the nearest hour.
# For example, 0:01 and 0:59 should both turn into 0:00. You can either alter the "Time of Day"
# column or create a new column and drop the old one, your choice.

# Hint: you can use the replace() function to alter the minute component of the datetime object

trips_final_df["Time of Day"] = trips_final_df["start_time"].apply(lambda x: x.split()[1][:-2]+"00").astype(datetime64).apply(lambda x: x.time())
# trips_final_df["Time of Day"] = trips_final_df["Time of Day"].apply(lambda x: x.time())
# trips_final_df

# Step 2a - Create a column called Date that gets the actual date (Month/Day/Year) seen
# in the start_time column. Similar process as part 1a, and you will find .date() helpful here

trips_final_df["Date"] = trips_final_df["start_time"].apply(lambda x: x.split()[0]).astype(datetime64)
# trips_final_df

# Step 2b- Using the 'Date' column, create a column called Day which has the day
# associated with the date (Monday, Tuesday, ..., Sunday). You can discard the 
# 'Date' column once you're done.

# Hint: the .weekday() function will be helpful here, but you cannot leave the final
# answer as an integer; make sure to convert to the actual name of the day!

# Note: 0 represents Monday, 1 represents Tuesday, ... , 6 represents Sunday
int2day = {0:"Monday",1:"Tuesday",2:"Wednesday",3:"Thursday",4:"Friday",5:"Saturday",6:"Sunday"}
trips_final_df["Day"] = trips_final_df["Date"].apply(lambda x: int2day[int(x.weekday())])
# trips_final_df

# Step 3-  Make a dataframe that just contains 'trip_id','duration','Time of Day', and 'Day'

new_trips = trips_final_df[['trip_id','duration','Time of Day', 'Day']]

"""Your dataset is now complete! For the first line plot, you will be visualizing the number of trips that occur during each hour of the day, for all 7 days. To do so, you will be creating 2 line charts using Seaborn (sns).

### This is important: using only Matplotlib will NOT result in full credit

It is crucial that **your first line graph** contains the following features:
1. The X-axis should be labelled "Time of Day", is of type datetime and ranges from 00:00 (midnight) to 23:00 (11 pm)
2. The Y-axis should be labelled "# of Rides".
3. There should be a title called "# of Rides for each Time of Day"
4. There are 7 lines for each day of the week, clearly labeled and differentiated, **containing markers**.
5. Again, must be completed using Seaborn (sns)

"""

# First Plot:

# For this plot, create a dataframe that does not the duration column
plot1_df = new_trips[['trip_id','Day','Time of Day']]


# Now, alter this dataframe to have 3 columns, IN THIS ORDER:
# 'day', 'Time of Day', 'Count'. Count will keep track of how many rides occured 
# during the particular hour of that day. Using Pandas for aggregation is strongly recommended.

plot1_df = plot1_df.groupby(["Time of Day","Day"]).count()
plot1_df.reset_index(inplace=True)
plot1_df = plot1_df.rename({"trip_id":"Count", "Day":"day"}, axis=1)
plot1_df = plot1_df[["day", "Time of Day", "Count"]]
plot1_df["Time of Day"] = plot1_df["Time of Day"].astype(str)

plot1_df.info()

# Now you have everything you need to create and display a line graph that
# meets all the necessary criteria.
plot1_df_wide = plot1_df.pivot("Time of Day", "day", "Count")
plt.figure(figsize=(20,5)) 
sns.lineplot(data=plot1_df_wide).set(title="# of Rides for each Time of Day",ylabel="# of Rides")

"""Very interesting. Now for the second line plot, you will be visualizing the average duration of a ride that occurs during each hour of the day, for all 7 days. 

As a reminder, **your second line graph** should contain the following features:
1. The X-axis should be the same as last time
2. The Y-axis should be labelled "Average Duration".
3. There should be a title called "Average Duration of an Indego Bike Ride for each Time of Day"
4. There are 7 lines for each of the 7 days
"""

# Second Plot:

# For this plot, create a dataframe that does not the trip_id column
plot2_df = new_trips[['duration','Day','Time of Day']].groupby(["Time of Day","Day"]).mean()
plot2_df.reset_index(inplace=True)
plot2_df = plot2_df.rename({"duration":"Average Duration"}, axis=1)
# plot1_df = plot1_df[["day", "Time of Day", "Count"]]
plot2_df["Time of Day"] = plot2_df["Time of Day"].astype(str)
plot2_df
# Now, alter this dataframe to have 3 columns, IN THIS ORDER:
# 'day', 'Time of Day', 'Average Duration'. Average Duration is the mean duration time
# for the particular hour of that day. 


# Now just plot the graph

plot2_df_wide = plot2_df.pivot("Time of Day", "Day", "Average Duration")
plt.figure(figsize=(20,5)) 
sns.lineplot(data=plot2_df_wide).set(title="Average Duration of an Indego Bike Ride for each Time of Day",ylabel="Average Duration")

"""## Part 4: Working with Text Data [9 points]

Now, let's switch gears and try to text-based analysis. Textual data is complex, but can also be used to generate extremely interpretable results, making it both valuable and interesting. 

Throughout this section, we will attempt to answer the following question:

**According to the `reviews_df` dataframe, what do the reviews for some of the most popular restaurants in Philadelphia look like?**

###4.1 Tokenizing the text [2 points]

We are going to split the contents of the `reviews` column from `reviews_df` into a list of words. We will use the **nltk** library, which contains an extensive set of tools for text processing. Now, this homework would be interminably long if we went into all the details of nltk. Thus, we are only going to use the following components of the library:
- `nltk.word_tokenize()`: a function used to tokenize text
- `nltk.corpus.stopwords`: a list of commonly used words such as "a", "an","in" that are often ignored in text analysis

Note that for this question, we didn't have to clean the text data first as our original dataset was well-formatted. However, in practice, we would typically clean the text first using regular expressions (regex). Keep this in mind as you work on the project later on in the semester.

**TODO:** Perform the following tasks:
- Use **nltk.corpus.stopwords** to create a set containing the most common English stopwords.
- Implement the function **tokenized_content(content)**, which takes in a string and does the following:
1. Tokenize the text
2. Keep tokens that only contain alphabetic characters (i.e. tokens with no punctuation)
3. Convert each token to lowercase
4. Remove stopwords (commonly used words such as "a", "an", "in")
"""

# We've imported the nltk library and created the stopwords set for you
import re
from string import punctuation

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = set(stopwords.words('english'))

# TODO: tokenize and flatten 
# (create a tokenized_content function that performs the steps listed above)

def tokenized_content(content):
  content = nltk.word_tokenize(content)
  tokens = []
  for c in content:
    if c.isalpha():
      token = c.lower()
      if token and token not in stopwords:
        tokens.append(token)
  return tokens

tokenized_content("THIS TEXT WILL BE LOWERCASED. THIS WON'T: ßßß")

""" **TODO**: Now perform the following tasks: 
- We are interested in reviews for the **most reviewed** restaurant in `reviews_df`. Using Counter() to find this restaurant is recommended. Store the name of the restaurant in the `most_reviewed_restaurant` variable, and use it to create `restaurant_reviews_df`, a subset of `reviews_df` only containing instances from the restaurant in question
- From there, extract the `review` column of `reviews_df` as a list called `reviews`. 
- Apply your `tokenize_content()` function to each item in the list `reviews`. Call the resultant list `top_tokens_list`. 
- Flatten the list `top_tokens_list`, and call the resultant list `top_tokens`. The autograder will be examining the contents of this list. (Flatten using list comprehension)

For futher assistance, here is the documentation for Counter() objects: 

https://docs.python.org/2/library/collections.html#counter-objects
"""

from collections import Counter
# TODO: find the most reviewed restaurant, use that to make restaurant_reviews_df,
#       extract the reviews, use your function to make the token list, and flatten it

most_reviewed_restaurant = list(Counter(reviews_df['restaurant']).items())[0]
for r in Counter(reviews_df['restaurant']).items():
  if r[1] > most_reviewed_restaurant[1]:
    most_reviewed_restaurant = r
most_reviewed_restaurant = most_reviewed_restaurant[0]
print(most_reviewed_restaurant)
restaurant_reviews_df = reviews_df[reviews_df['restaurant'] == most_reviewed_restaurant]
# restaurant_reviews_df
reviews = restaurant_reviews_df.review.tolist()
top_tokens_list = [tokenized_content(review) for review in reviews]
top_tokens = []
for ele in top_tokens_list:
  for item in ele:
    top_tokens.append(item)

# 2 points
grader.grade(test_case_id = 'test_top_tokens', answer = top_tokens)

"""
### 4.2 Most Frequent Words [2 points]
**TODO**: Now, find the 10 most common words amongst the content of `top_tokens`. Return this as a list of `(word, count)` tuples, in descending order of `count`. Store this variable in `top_ten_words`

**Hint**: We again recommend using `Counter` in this question."""

# TODO: From top_tokens, find the ten most frequent words
top_ten_words = Counter(top_tokens).most_common(10)

# 2 points
grader.grade(test_case_id = 'test_top_most_common', answer = top_ten_words)

"""### 4.3 Word Clouds [5 points]

Before we move on from this dataset, let's visualize our results using a word cloud.

**TODO**: Create a word cloud containing all the words in the list `top_tokens` (created in part 4.1). [The WordCloud documentation](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) contains instructions on how to do this. 

*Please make sure your wordcloud has a **lavender** background color.*

We will be going through your notebooks and **manually grading** your word cloud. This is worth 5 points. 
"""

# TODO: generate your word cloud, making sure it meets the requirements above
import wordcloud
wordcloud = wordcloud.WordCloud(background_color="lavender")
wordcloud.generate(" ".join(top_tokens))
plt.axis("off")
plt.imshow(wordcloud, interpolation="bilinear")
plt.show()

"""# HW Submission

<br>
<center><img src = "https://i.imgflip.com/79knab.jpg" width= "500" align ="center"/></center>
<br>

Congratulations on finishing this homework! The good news is that similar to HW1, you basically know your score when you submit to Gradescope. 
However, this time, we will be manually grading your lineplots and wordclouds, so the autograder score is not final! Remember that we will also be checking for plagiarism, so please make sure to cite your sources (if any) by commenting the urls / links you looked at.

Before you submit on Gradescope (you must submit your notebook to receive credit):

1.   Please rerun your notebook on Colab by clicking "Restart and Run-All", and make sure there is nothing wrong with your notebook.
2.   **Double check that you have the correct PennID (all numbers) saved in the autograder**. 
3. Make sure you've run all the PennGrader cells and have received a score.
4. Go to the "File" tab at the top left, and click "Download .ipynb" + "Download .py". Please name the `.ipynb` and `.py` files **"homework2.ipynb"** and **"homework2.py"** respectively. Then, upload both the `.py` and `.ipynb` files to Gradescope. 

###Be sure to name your files correctly!!!

**Please let the course staff know ASAP if you have any issues submitting.**
"""